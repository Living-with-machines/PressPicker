{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step01: Filtering and processing newspaper titles\n",
    "\n",
    "This is step 01 of the **PressPicker** tool.\n",
    "\n",
    "In this notebook, we read in the title-level British Library newspaper dataset https://doi.org/10.23636/1136, filter it by date, geography, and to only undigitised titles. \n",
    "\n",
    "We process free-text data describing title name changes through time to create structured data.\n",
    "\n",
    "We read in item-level data for microfilm and hardcopy holdings of these titles, re-form this data into time series, and join it to the title-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always show all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in newspaper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in newspaper titles spreadsheet from path2newspaper_titles\n",
    "# This file is publically available at https://doi.org/10.23636/1136\n",
    "path2newspaper_titles = os.path.join(\"datasets\", \"BritishAndIrishNewspapers_20191118.xlsx\")\n",
    "sheet_name = \"Title List\"\n",
    "\n",
    "titles_orig_read = pd.read_excel(path2newspaper_titles, sheet_name=sheet_name, dtype='str')\n",
    "titles = titles_orig_read\n",
    "titles.head()\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering newspaper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- remove already digitised titles\n",
    "# Online status: if BNA or BURNEY, they are already digitised and no need to keep them in the dataframe\n",
    "print(titles[\"Online status\"].value_counts())\n",
    "print(\"-------\")\n",
    "print(\"Total number of titles: %s\" % len(titles.index))\n",
    "titles = titles[pd.isnull(titles[\"Online status\"]) & pd.isnull(titles[\"Link to British Newspaper Archive\"])]\n",
    "print(\"Total number of undigitised titles:  %s\" % len(titles.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean strings when doing filtering \n",
    "def cleanuplist(input_list):\n",
    "    input_list = [re.sub(r\"\\s*\\|\\s*\", \"|\", x) if (not pd.isnull(x))\n",
    "            else x for x in input_list]\n",
    "    input_list = [re.sub(r\"\\s+\", \" \", x) if (not pd.isnull(x)) else x for x in input_list]\n",
    "\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- cleanup `Country of publication`\n",
    "titles['Country of publication'] = cleanuplist(titles['Country of publication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of `Country of publication`\n",
    "print(\"================ Unique values\")\n",
    "print(titles['Country of publication'].unique())\n",
    "print(\"\\n\\n================ Unique values and their counts\")\n",
    "print(titles['Country of publication'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries to include\n",
    "countries_include_list = [\n",
    "    'England', \n",
    "    'Scotland',\n",
    "    'Wales',\n",
    "    'England|Wales',\n",
    "    'England|Scotland',\n",
    "    'England|United Kingdom',\n",
    "    'United Kingdom',\n",
    "    'Scotland|United Kingdom'\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of titles (before filtering): %s\" % len(titles.index))\n",
    "titles = titles[titles['Country of publication'].isin(countries_include_list)]\n",
    "print(\"Total number of titles (after filtering):  %s\" % len(titles.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct dates and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for dates in form '1895|1899', '1981|Continuing', 'Continuing'\n",
    "# Assume: \"continuing\" means still being published, replace date with current_date\n",
    "def correct_dates_titles(mydate, current_date=2021):\n",
    "    if pd.notna(mydate):\n",
    "        # if continu* is in mydate, replace it with current_date\n",
    "        if \"continu\" in str(mydate).lower():\n",
    "            mydate = current_date\n",
    "        \n",
    "        if \"|\" in str(mydate):\n",
    "            return int(mydate.split(\"|\")[0])\n",
    "        else:\n",
    "            return int(mydate)\n",
    "        \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date is used to deal with \"Continuing\" fields in dates\n",
    "current_date = 2021\n",
    "\n",
    "titles['First date held'] = titles['First date held'].apply(correct_dates_titles, args=(current_date,))\n",
    "titles['Last date held'] = titles['Last date held'].apply(correct_dates_titles, args=(current_date,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by date\n",
    "earliest_date_filter = 1780\n",
    "latest_date_filter = 1918\n",
    "\n",
    "# Filter where overlap with our date span, but EXCLUDE where no first or last date held\n",
    "def filter_by_dates (df):\n",
    "    return df[((df['First date held'] <= latest_date_filter)| pd.isnull(df['First date held'])) & \n",
    "              ((df['Last date held']  >= earliest_date_filter) | pd.isnull(df['Last date held'])) & \n",
    "              ~(pd.isnull(df['First date held']) & pd.isnull(df['Last date held']))]\n",
    "\n",
    "print(\"Total number of titles (before filtering): %s\" % len(titles))\n",
    "# Filter titles by dates\n",
    "titles = filter_by_dates(titles)\n",
    "print(\"Total number of titles (after filtering): %s\" % len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Checking against Original Request -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Hardcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in hardcopy spreadsheet\n",
    "items_hardcopy_path = os.path.join(\"datasets\", \"titles_ALLnotDigitised_31_10_2019 - RESULTS.xlsx\")\n",
    "items_hardcopy = pd.read_excel(items_hardcopy_path, dtype='str')\n",
    "\n",
    "# rename some columns\n",
    "items_hardcopy.rename(columns={\"ID\": \"ID_HC\"}, inplace=True)\n",
    "\n",
    "print(\"Total number of Hard Copies, after filtering: %s\" % len(items_hardcopy.index))\n",
    "items_hardcopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter hardcopies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where Sublibrary == HMNTS (these are positive microfilm - which are excluded in Press Picker)\n",
    "items_hardcopy = items_hardcopy[items_hardcopy['sublibrary'] != 'HMNTS']\n",
    "print(\"Total number of Hard Copies, after removing positive microfilms: %s\" % len(items_hardcopy.index))\n",
    "items_hardcopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== merge titles with HCs, we use Title.ID and ID_HC\n",
    "titles_hardcopy = pd.merge(titles, items_hardcopy, left_on='Title.ID', right_on='ID_HC', how='outer')\n",
    "print(\"Title's shape: {}\".format(titles.shape))\n",
    "print(\"HC shape: {}\".format(items_hardcopy.shape))\n",
    "print(\"Title-HC merge: {}\".format(titles_hardcopy.shape))\n",
    "titles_hardcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title.ID can be NaN since some of the titles have been already filtered out (see the cells above) - remove these rows\n",
    "titles_hardcopy = titles_hardcopy[~pd.isnull(titles_hardcopy['Title.ID'])]\n",
    "print(\"Title-HC merge, after filtering: {}\".format(titles_hardcopy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Microfilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in microfilm spreadsheet\n",
    "items_microfilm_path = os.path.join(\"datasets\", \"nmf-items-1911-0320.xlsx\")\n",
    "items_microfilm = pd.read_excel(items_microfilm_path,\n",
    "                                sheet_name='Sheet3',\n",
    "                                dtype='str')\n",
    "\n",
    "items_microfilm = items_microfilm[~pd.isnull(items_microfilm['bib'])]\n",
    "print(\"MF shape: {}\".format(items_microfilm.shape))\n",
    "items_microfilm.head()\n",
    "print(\"Filter out titles with no canNo.\")\n",
    "items_microfilm = items_microfilm[~pd.isnull(items_microfilm['canNo'])]\n",
    "print(\"MF shape: {}\".format(items_microfilm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microfilm can numbers are all numbers except for '0180B' which causes filtering problems\n",
    "# since item '0180B' is out of our date range (1952), exclude it\n",
    "items_microfilm = items_microfilm[items_microfilm['canNo'] != '0180B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== merge titles with MFs, we use Title.ID and bib\n",
    "titles_mf = pd.merge(titles, items_microfilm, left_on='Title.ID', right_on='bib', how='outer')\n",
    "print(\"Title-MF merge: {}\".format(titles_mf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title.ID can be NaN since some of the titles have been already filtered out (see the cells above) - remove these\n",
    "titles_mf = titles_mf[~pd.isnull(titles_mf['Title.ID'])]\n",
    "print(\"Title-MF merge, after filtering: {}\".format(titles_mf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change canNo column to numbers, so can filter on this later\n",
    "titles_mf['canNo'] = titles_mf['canNo'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_mf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create \"connectivity\": titles storing the IDs of connected titles\n",
    "\n",
    "In this section, we create structured data to connect titles that are related by name changes through time. \n",
    "\n",
    "**Steps:**\n",
    "1. Preprocess `Publication title`, `Preceding titles`, `Succeeding titles` and `General area of coverage` using `clean_succeed_preceding` function (see below). Note that, in the first three cases (i.e., `Publication/Preceding/Succeeding titles`), if title(s) exist, they are converted into a Python list; otherwise, `nan` will be stored. \n",
    "2. Here, we find the `Title.ID`s of preceding/succeeding titles of each `Publication title`. We use the Python list created in Step 1 (and skip those that are `nan`). We only consider the preceding/succeeding titles with similar `general_area_of_coverage_str` to the main publication title. See for example:\n",
    "\n",
    "```python\n",
    "found_titles = titles[(titles[\"publication_title_str\"] == ititle) & (titles[\"general_area_of_coverage_str\"] == ititle_General_area_of_coverage)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String matching between preceding/succeeding titles and publication titles\n",
    "# Introduces new 'connectivity' facet for titles storing the IDs of connected titles\n",
    "def clean_succeed_preceding(mytitle, makelist=True):\n",
    "    if 'nan' in str(mytitle):\n",
    "        return \"nan\"\n",
    "    mytitle = str(mytitle).lower()\n",
    "    mytitle = re.sub(\"no\\..*\", \"\", mytitle)\n",
    "    mytitle = re.sub(\"vol\\..*\", \"\", mytitle)\n",
    "    mytitle = re.sub(\"[0-9].*\", \"\", mytitle)\n",
    "    # Strip out 'the's\n",
    "    mytitle = re.sub(\"(^|\\s)the($|\\s)\", \"\", mytitle, flags=re.IGNORECASE)\n",
    "    mytitle = re.sub(r'Continued by:', '', mytitle, flags=re.IGNORECASE)\n",
    "    mytitle = re.sub(r'Continued in part by:', '', mytitle, flags=re.IGNORECASE)\n",
    "    mytitle = re.sub(r'continues:', '', mytitle, flags=re.IGNORECASE)\n",
    "    mytitle = re.sub(r'continues in part:', '', mytitle, flags=re.IGNORECASE)\n",
    "    mytitle = re.sub(re.compile(r'\\s+'), '', mytitle)\n",
    "    mytitle = re.sub(re.compile(r' \\(.*\\)'), '', mytitle)\n",
    "   \n",
    "    if str(mytitle) == '':\n",
    "        return \"nan\"\n",
    "    elif makelist:\n",
    "        mytitle = re.sub('[%s]' % re.escape(string.punctuation.replace(\"|\", \"\")), '', mytitle)\n",
    "        return mytitle.split(\"|\")\n",
    "    else:\n",
    "        mytitle = re.sub('[%s]' % re.escape(string.punctuation.replace(\"|\", \"\")), '', mytitle)\n",
    "        return mytitle\n",
    "\n",
    "titles[\"connectivity\"] = titles[\"Title.ID\"]\n",
    "titles[\"connectivity\"] = ''\n",
    " \n",
    "titles[\"publication_title_str\"] = titles[\"Publication title\"]\n",
    "titles[\"preceding_title_str\"] = titles[\"Preceding titles\"]\n",
    "titles[\"succeeding_title_str\"] = titles[\"Succeeding titles\"]\n",
    " \n",
    "titles[\"publication_title_str\"] = titles[\"publication_title_str\"].apply(clean_succeed_preceding, makelist=False)\n",
    "titles[\"preceding_title_str\"] = titles[\"preceding_title_str\"].apply(clean_succeed_preceding, makelist=True)\n",
    "titles[\"succeeding_title_str\"] = titles[\"succeeding_title_str\"].apply(clean_succeed_preceding, makelist=True)\n",
    " \n",
    "titles[\"general_area_of_coverage_str\"] = titles[\"General area of coverage\"].apply(clean_succeed_preceding, makelist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for irow in range(len(titles)):\n",
    "    connectivity = []       \n",
    "    if not titles[\"preceding_title_str\"][irow] == 'nan':\n",
    "        for ititle in titles[\"preceding_title_str\"][irow]:\n",
    "            ititle_General_area_of_coverage = titles[\"general_area_of_coverage_str\"][irow]\n",
    "            # checking if title matches have the same General.area.of.coverage\n",
    "            found_titles = titles[(titles[\"publication_title_str\"] == ititle) & (titles[\"general_area_of_coverage_str\"] == ititle_General_area_of_coverage)]\n",
    "            if len(found_titles) > 0:\n",
    "                connectivity.extend(found_titles[\"Title.ID\"].values.tolist())\n",
    "    if not titles[\"succeeding_title_str\"][irow] == 'nan':\n",
    "        for ititle in titles[\"succeeding_title_str\"][irow]:\n",
    "            ititle_General_area_of_coverage = titles[\"general_area_of_coverage_str\"][irow]\n",
    "            found_titles = titles[(titles[\"publication_title_str\"] == ititle) & (titles[\"general_area_of_coverage_str\"] == ititle_General_area_of_coverage)]\n",
    "            if len(found_titles) > 0:\n",
    "                connectivity.extend(found_titles[\"Title.ID\"].values.tolist())\n",
    "    if len(connectivity) > 0:\n",
    "        connectivity = list(dict.fromkeys(connectivity))\n",
    "        titles.at[irow, \"connectivity\"] = \",\".join(connectivity)\n",
    "    else:\n",
    "        titles.at[irow, \"connectivity\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Sanity check, Number of HCs and Mfs -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries dataframe\n",
    "\n",
    "Create timeseries data from the microfilm and hardcopy holdings for visualising in Step 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new timeseries dataframe for items\n",
    "earliest_date_titles = pd.to_numeric(titles['First date held']).min()\n",
    "latest_date_titles = pd.to_numeric(titles['Last date held']).max()\n",
    "\n",
    "full_date_span = list(range(int(earliest_date_titles), int(latest_date_titles) + 10))\n",
    "\n",
    "timeseries_items = pd.DataFrame(columns=full_date_span)\n",
    "# add empty column at end for items with unknown dates\n",
    "timeseries_items[\"Total_canNos_below_4000\"] = \"\"\n",
    "\n",
    "# HC: Hard Copy, MF: Microfilm\n",
    "timeseries_items_hc = timeseries_items.copy()\n",
    "timeseries_items_mf = timeseries_items.copy()\n",
    "timeseries_items_hc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dates_001(mydate):\n",
    "    if 'nan' in str(mydate):\n",
    "        return \"Unknown\"\n",
    "    elif \"|\" in str(mydate):\n",
    "        return mydate.split(\"|\")[0]\n",
    "    elif \"/\" in str(mydate):\n",
    "        return mydate.split(\"/\")[0]\n",
    "    else:\n",
    "        return mydate[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries for Hardcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill timeseries_items_hc dataframe\n",
    "unique_title_hc = titles_hardcopy['ID_HC'].unique() \n",
    "unique_title_hc = unique_title_hc[~pd.isna(unique_title_hc)]\n",
    "\n",
    "id_hc_count = []\n",
    "for irow in unique_title_hc:\n",
    "    titles_hardcopy_row = titles_hardcopy[titles_hardcopy['ID_HC'] == irow]\n",
    "    id_hc_count.append([irow, \n",
    "                        len(titles_hardcopy_row), \n",
    "                        titles_hardcopy_row['Chron I'].apply(correct_dates_001).to_list()])\n",
    "    timeseries_items_hc.at[irow, :] = 0\n",
    "    for idate in id_hc_count[-1][-1]:\n",
    "        if idate == 'Unknown':\n",
    "            pass\n",
    "        elif len(idate) > 4:\n",
    "            idate = idate.replace(\" \", \"\")\n",
    "            first_date_hc = int(idate[:4])\n",
    "            last_date_hc = int(idate[-4:])\n",
    "            year_difference = last_date_hc - first_date_hc\n",
    "            fraction_per_year = 1. / (year_difference + 1.) # If hc item covers a time span, timeseries is populated with fraction per year\n",
    "            for iyear in range(first_date_hc, last_date_hc + 1):\n",
    "                timeseries_items_hc.at[irow, iyear] += fraction_per_year\n",
    "        else:\n",
    "            timeseries_items_hc.at[irow, int(idate)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_items_hc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_names = list(range(int(earliest_date_filter), int(latest_date_filter + 1)))\n",
    "columns_to_keep_names.append('Total_canNos_below_4000')\n",
    "timeseries_items_hc = timeseries_items_hc.loc[:,columns_to_keep_names]\n",
    "timeseries_items_hc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort timeseries_items_hc based on #HCs\n",
    "idx_hc = timeseries_items_hc.sum(axis=1).sort_values(ascending=False).index\n",
    "timeseries_items_hc = timeseries_items_hc.loc[idx_hc]\n",
    "timeseries_items_hc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries for Microfilms\n",
    "Create timeseries data for microfilms and create count of acetate microfilms (important to know for digitisation, and and an element of the visualisation in Step 02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill timeseries_items_mf dataframe\n",
    "# Microfilm records can cover a date span (between 'startDate' and 'endDate'): splits the volume count (1) over the date span \n",
    "# Excludes records with unknown dates (these also have no Can location information) - these are notes in Aleph database rather than referring to physcial resources\n",
    "unique_title_mf = titles_mf['bib'].unique() \n",
    "unique_title_mf = unique_title_mf[~pd.isna(unique_title_mf)]\n",
    "\n",
    "bib_mf_count = []\n",
    "# BL microfilms with can number below 4000 are likely made of acetate. This is important to know for digitisation, so the count in recorded for each title. The count is visualised in Step 02.\n",
    "can_acetate_threshold = 4000\n",
    "for irow in unique_title_mf:\n",
    "    titles_mf_row = titles_mf[titles_mf['bib'] == irow]\n",
    "    bib_mf_count.append([irow, \n",
    "                         len(titles_mf_row), \n",
    "                         titles_mf_row[['startDate', 'endDate']].applymap(correct_dates_001).values.tolist()])\n",
    "    timeseries_items_mf.at[irow, :] = 0\n",
    "        \n",
    "    # Only count'Total_canNos_below_4000' for records between 1780 - 1918\n",
    "    if not titles_mf_row[(titles_mf_row['canNo'] < can_acetate_threshold)][['startDate']].applymap(correct_dates_001).empty:\n",
    "        startDates_for_acetate_records = titles_mf_row[(titles_mf_row['canNo'] < can_acetate_threshold)][['startDate']].applymap(correct_dates_001)\n",
    "        timeseries_items_mf.at[irow, f\"Total_canNos_below_{can_acetate_threshold}\"] += len(startDates_for_acetate_records[(startDates_for_acetate_records['startDate'].astype(int) >= int(earliest_date_filter)) & (startDates_for_acetate_records['startDate'].astype(int) <= int(latest_date_filter))])\n",
    "    \n",
    "    for idate in bib_mf_count[-1][-1]:\n",
    "        if idate[0] == 'Unknown':\n",
    "            pass\n",
    "        elif (idate[1] == 'Unknown') | (idate[0] == idate[1]) :\n",
    "            timeseries_items_mf.at[irow, int(idate[0])] += 1\n",
    "        else:\n",
    "            year_difference = int(idate[1]) - int(idate[0])\n",
    "            fraction_per_year = 1. / (year_difference + 1.)\n",
    "            for iyear in range(int(idate[0]), int(idate[1]) + 1):\n",
    "                try:\n",
    "                    timeseries_items_mf.at[irow, iyear] += fraction_per_year\n",
    "                except:\n",
    "                    # Throws an exception where date is outside date range, and therefore not covered by timeseries_items_mf\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_to_keep_names)\n",
    "timeseries_items_mf = timeseries_items_mf.loc[:,columns_to_keep_names]\n",
    "timeseries_items_mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort timeseries_items_mf based on #MFs\n",
    "idx_bib_mf = timeseries_items_mf.sum(axis=1).sort_values(ascending=False).index\n",
    "timeseries_items_mf = timeseries_items_mf.loc[idx_bib_mf]\n",
    "timeseries_items_mf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell exports datasets that are used in Step02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = os.path.join(\"datasets\", \"dynamic_io\")\n",
    "\n",
    "if not os.path.isdir(parent_path): \n",
    "    os.makedirs(parent_path)\n",
    "    \n",
    "titles.to_csv(os.path.join(parent_path, \"titles.csv\"))\n",
    "timeseries_items_mf.to_csv(os.path.join(parent_path, \"timeseries_items_mf.csv\"))\n",
    "timeseries_items_hc.to_csv(os.path.join(parent_path, \"timeseries_items_hc.csv\"))\n",
    "titles_hardcopy.to_csv(os.path.join(parent_path, \"titles_hc.csv\"))\n",
    "titles_mf.to_csv(os.path.join(parent_path, \"titles_mf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
